{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HateBot Machine Learning\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Hate speech is something that is a big issue nowadays especially with social media like Twitter. Negativity that comes out of it can take a toll on a personâ€™s mental health. To minimize the damage of that, the HateBot can return if a tweet is hate speech or not by being given a Tweet. This way we can ensure that the world becomes if not a better place then at least a place where we can influence the amount of negativity we get.\n",
    "\n",
    "Note: This is the second notebook of this project. The EDA is conducted in a separate Jupyter notebook that can be found in the submission.\n",
    "\n",
    "## 2. Setup\n",
    "\n",
    "In this part of the notebook, I am going to do the importing of libraries/modules and the dataset and setup the global settings needed for the figures.\n",
    "\n",
    "### 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.1.3\n",
      "matplotlib version: 3.3.2\n",
      "scattertext version: 0.1.2\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scattertext as st\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "\n",
    "print('pandas version:', pd.__version__)\n",
    "print('matplotlib version:', matplotlib.__version__)\n",
    "print('scattertext version:', st.__version__)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Importing the dataset that will be used\n",
    "\n",
    "The dataset is stored in a CSV format (comma-separated values file) in the file `HateBotDataset.csv`. This dataset is already cleaned in the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CleanedHateBotDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Check the data in the dataset\n",
    "To be sure, we want to see the first five entries of the dataset to make sure that we have loaded the correct file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>marked</th>\n",
       "      <th>is_hate_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT  As a woman you shouldnt complain about cl...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT  boy dats coldtyga dwn bad for cuffin dat ...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT  Dawg RT  You ever fuck a bitch and she st...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT   she look like a tranny</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT  The shit you hear about me might be true ...</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  neither  class                                              tweet  \\\n",
       "0      3        3      2   RT  As a woman you shouldnt complain about cl...   \n",
       "1      3        0      1   RT  boy dats coldtyga dwn bad for cuffin dat ...   \n",
       "2      3        0      1   RT  Dawg RT  You ever fuck a bitch and she st...   \n",
       "3      3        1      1                        RT   she look like a tranny   \n",
       "4      6        0      1   RT  The shit you hear about me might be true ...   \n",
       "\n",
       "   marked  is_hate_speech  \n",
       "0       0           False  \n",
       "1       3            True  \n",
       "2       3            True  \n",
       "3       2            True  \n",
       "4       6            True  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Selection of an algorithm for the Machine Learning\n",
    "\n",
    "![alt text](https://scikit-learn.org/stable/_static/ml_map.png \"Choosing the right algorithm to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms that will be used for this challenge are Support Vector Classification (SVC) and Naive Bayes. The following algorithms were selected because we have our sample size (less than 100k) and because we are using text data (because the Tweets are text).\n",
    "Naive bayes can be very good with low amounts of data. Decision trees work better with lots of data compared to Naive Bayes but our sample size is not big enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data description\n",
    "\n",
    "From the EDA phase, it has been decided that we are going to train via the `tweet` column (data type: `string`) and the `is_hate_speech` (data type: `Boolean`) column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine learning using a Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training using a SVC with a CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer converts a collection of text documents to a matrix of token counts. This is needed because machine learning algorithms cannot run on raw text data. One limitation of these methods is that the vocabulary can become very large.\n",
    "\n",
    "#### 3.1.1 Get all the words that are in all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a\n",
      "bitch\n",
      "RT\n",
      "the\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "d = Counter(\" \".join(df.tweet).split(\" \")).items()\n",
    "most_used_words = dict(sorted(d, key=lambda item: item[1], reverse=True))\n",
    "words = {key for key, val in most_used_words.items()} \n",
    "\n",
    "count = 0\n",
    "for elem in iter(most_used_words):\n",
    "    if count == 6:\n",
    "        break\n",
    "    print(elem)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Create and fit the CountVectorizer\n",
    "Here we tokenize the words and build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countVectorizer = CountVectorizer()\n",
    "countVectorizer.fit(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Encode all the words from the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34011, 28861)\n"
     ]
    }
   ],
   "source": [
    "countVector = countVectorizer.transform(words)\n",
    "print(countVector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 34011 rows of data with 28861 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Transform every tweet into a pandas Series\n",
    "\n",
    "We add a new column called vectorised_words which has for every row in the dataset the tweet itself but after it was transformed via our count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vectorised_words'] = countVectorizer.transform(df.tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>marked</th>\n",
       "      <th>is_hate_speech</th>\n",
       "      <th>vectorised_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT  As a woman you shouldnt complain about cl...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>(0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT  boy dats coldtyga dwn bad for cuffin dat ...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT  Dawg RT  You ever fuck a bitch and she st...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT   she look like a tranny</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT  The shit you hear about me might be true ...</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>(0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  neither  class                                              tweet  \\\n",
       "0      3        3      2   RT  As a woman you shouldnt complain about cl...   \n",
       "1      3        0      1   RT  boy dats coldtyga dwn bad for cuffin dat ...   \n",
       "2      3        0      1   RT  Dawg RT  You ever fuck a bitch and she st...   \n",
       "3      3        1      1                        RT   she look like a tranny   \n",
       "4      6        0      1   RT  The shit you hear about me might be true ...   \n",
       "\n",
       "   marked  is_hate_speech                                   vectorised_words  \n",
       "0       0           False    (0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...  \n",
       "1       3            True    (0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...  \n",
       "2       3            True    (0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...  \n",
       "3       2            True    (0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...  \n",
       "4       6            True    (0, 2290)\\t1\\n  (0, 2735)\\t1\\n  (0, 2799)\\t1...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['vectorised_words'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training the SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Create the needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_1 = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = countVectorizer.transform(df.tweet.values)\n",
    "y = df['is_hate_speech'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the test_size parameter, we keep 30% of the data for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Fit the SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84      1239\n",
      "           1       0.97      0.97      0.97      6196\n",
      "\n",
      "    accuracy                           0.95      7435\n",
      "   macro avg       0.91      0.90      0.91      7435\n",
      "weighted avg       0.95      0.95      0.95      7435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svc_1.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.947679892400807\n",
      "Precision: 0.9679290894439968\n",
      "Recall: 0.9693350548741123\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this model is quite succesfull - it's accuracy is almost at 95% which in our case is more than sufficient because the initial target accuracy was over 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Hyperparameter tuning\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. To reach to the highest performance possible of a model, we need to try different hyperparameters. To achieve these, GridSearch will be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 24.6min finished\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)\n",
    "estimations = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, gamma=0.01, kernel='sigmoid')\n"
     ]
    }
   ],
   "source": [
    "print(estimations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the GridSearch, we can see that we should use SVC(C=10, gamma=0.01, kernel='sigmoid'. Now we will make that so that we have the best possible SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1094  145]\n",
      " [ 215 5981]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      1239\n",
      "           1       0.98      0.97      0.97      6196\n",
      "\n",
      "    accuracy                           0.95      7435\n",
      "   macro avg       0.91      0.92      0.91      7435\n",
      "weighted avg       0.95      0.95      0.95      7435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "print(confusion_matrix(y_test,grid_predictions))\n",
    "print(classification_report(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4.1 Creating the hypertuned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      1239\n",
      "           1       0.98      0.97      0.97      6196\n",
      "\n",
      "    accuracy                           0.95      7435\n",
      "   macro avg       0.91      0.92      0.91      7435\n",
      "weighted avg       0.95      0.95      0.95      7435\n",
      "\n",
      "Accuracy: 0.9515803631472763\n",
      "Precision: 0.9763303950375449\n",
      "Recall: 0.9653001936733376\n"
     ]
    }
   ],
   "source": [
    "svc_1 = SVC(C=10, gamma=0.01, kernel='sigmoid')\n",
    "X = countVectorizer.transform(df.tweet.values)\n",
    "y = df['is_hate_speech'].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=46)\n",
    "svc_1.fit(X_train, y_train)\n",
    "y_pred = svc_1.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the hyperparameter tuning, we had the following results:\n",
    "\n",
    "```\n",
    "Accuracy: 0.94\n",
    "Precision: 0.96\n",
    "Recall: 0.96\n",
    "```\n",
    "With it, we had the following results:\n",
    "```\n",
    "Accuracy: 0.95\n",
    "Precision: 0.97\n",
    "Recall: 0.96\n",
    "```\n",
    "We can see that there is an increase in the Accuracy and precision metrics of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 Save as a pickle\n",
    "\n",
    "For the deployment part of the project, we will be having a Django web application where the user can enter the Tweet they want to get checked and then return if the Tweet is hate or not. To do so, we need the already trained model. For this, I have decided to use pickle because it is very easy to integrate with Django. \n",
    "It is used for serializing and de-serializing of Python object structures. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network. Later on, this character stream can then be retrieved and de-serialized back to a Python object. \n",
    "\n",
    "Now we will extract this SVC model to a pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickl = {\n",
    "    'vectorizer': countVectorizer,\n",
    "    'regressor': svc_1\n",
    "}\n",
    "pickle.dump( pickl, open( \"HateBotModels.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training using a SVC with a HashingVectorizer\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    "- it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "- it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    "- it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    "\n",
    "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "\n",
    "- there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "- there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
    "- no IDF weighting as this would render the transformer stateful.\n",
    "\n",
    "*Source: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html* \n",
    "\n",
    "\n",
    "#### 3.3.1 Create the HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingVectorizer = HashingVectorizer(n_features=20)\n",
    "vector = hashingVectorizer.transform(df.tweet)\n",
    "df['vectorised_words'] = hashingVectorizer.transform(df.tweet.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.3.2 Create the needed variables and split the dataset into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_2 = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hashingVectorizer.transform(df.tweet.values)\n",
    "y = df['is_hate_speech'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.3.3 Fit the SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.3.4 Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1239\n",
      "           1       0.83      1.00      0.91      6196\n",
      "\n",
      "    accuracy                           0.83      7435\n",
      "   macro avg       0.42      0.50      0.45      7435\n",
      "weighted avg       0.69      0.83      0.76      7435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = svc_2.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333557498318762\n",
      "Precision: 0.8333557498318762\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when using the SVC with a HashingVectorizer, the accuracy of the model goes down. this can be explained by the fact that distinct tokens can be mapped to the same feature index using the `HashingVectorizer` in contract to using a `CountVectorizer`. However, this is still a very good result (because the accuracy is 0.83) even though it is worse than our previous one. It is important to note that here we have a perfect recall score of 1 - that means that all relevant instances were retrieved by the search (but says nothing about how many irrelevant instances were also retrieved)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5 Hyperparameter tuning\n",
    "\n",
    "As we saw in the previous model, hyperparameter tuning can improve the performance of our model. That's why we would also do it for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 22.5min finished\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)\n",
    "estimations = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n"
     ]
    }
   ],
   "source": [
    "print(estimations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the hypertuning, we can see that the best model is one with the following parameters:\n",
    "\n",
    "```SVC(C=1, gamma=1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1237]\n",
      " [   0 6196]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1239\n",
      "           1       0.83      1.00      0.91      6196\n",
      "\n",
      "    accuracy                           0.83      7435\n",
      "   macro avg       0.92      0.50      0.46      7435\n",
      "weighted avg       0.86      0.83      0.76      7435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test)\n",
    "print(confusion_matrix(y_test,grid_predictions))\n",
    "print(classification_report(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333557498318762\n",
      "Precision: 0.8333557498318762\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine learning using Naive Bayes\n",
    "\n",
    "In statistics, Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. They are among the simplest Bayesian network model but coupled with kernel density estimation, they can achieve higher accuracy levels.\n",
    "\n",
    "Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. \n",
    "\n",
    "## 4.1 Naive Bayes with a CountVectorizer\n",
    "### 4.1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Split the dataset into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = countVectorizer.transform(df.tweet.values).toarray()\n",
    "y = df['is_hate_speech'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Create the Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Calculate the metrics (accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655413584398117\n",
      "Precision: 0.9037777777777778\n",
      "Recall: 0.6563912201420271\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 Hyperparameter tuning \n",
    "\n",
    "We will try to tune the var_smoothing parameter. This variable artificially adds a user-defined value to the distribution's variance (whose default value is derived from the training data set). This essentially widens (or \"smooths\") the curve and accounts for more samples that are further away from the distribution mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[CV] var_smoothing=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................... var_smoothing=0.01, total=  40.2s\n",
      "[CV] var_smoothing=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................... var_smoothing=0.01, total=  29.5s\n",
      "[CV] var_smoothing=0.01 ..............................................\n",
      "[CV] ............................... var_smoothing=0.01, total=  24.8s\n",
      "[CV] var_smoothing=0.01 ..............................................\n",
      "[CV] ............................... var_smoothing=0.01, total=  23.2s\n",
      "[CV] var_smoothing=0.01 ..............................................\n",
      "[CV] ............................... var_smoothing=0.01, total=  24.2s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=  29.7s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=  24.2s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=  22.8s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=  21.5s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=  24.6s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=  24.1s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=  25.8s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=  29.4s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=  23.5s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=  24.2s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=  24.1s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=  23.8s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=  23.0s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=  22.5s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=  23.9s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=  24.2s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=  24.0s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=  26.7s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=  27.7s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=  25.8s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=  26.6s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=  27.2s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=  24.9s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=  29.2s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=  26.9s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=  29.0s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=  26.7s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=  29.2s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=  30.8s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=  28.6s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=  29.4s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=  27.8s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=  34.8s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=  36.1s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=  39.7s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=  44.8s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=  37.6s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=  45.3s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=  32.7s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=  30.4s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=  29.3s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=  28.8s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=  29.0s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=  31.3s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=  31.3s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=  30.7s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=  32.1s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=  37.9s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=  32.3s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=  34.7s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=  32.9s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=  35.5s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=  33.8s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=  33.4s\n",
      "[CV] var_smoothing=1e-13 .............................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................. var_smoothing=1e-13, total=  34.7s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=  31.9s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=  35.0s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=  31.8s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=  31.1s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=  33.0s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=  32.5s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=  34.2s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=  36.2s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=  31.4s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=  33.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed: 35.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': [0.01, 0.001, 0.0001, 1e-05, 1e-06,\n",
       "                                           1e-07, 1e-08, 1e-09, 1e-10, 1e-11,\n",
       "                                           1e-12, 1e-13, 1e-14, 1e-15]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n",
    "}\n",
    "clf = GridSearchCV(nb_classifier, parameters, cv=5, verbose=2)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_var_smoothing</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030201</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'var_smoothing': 0.01}</td>\n",
       "      <td>0.825701</td>\n",
       "      <td>0.824491</td>\n",
       "      <td>0.829131</td>\n",
       "      <td>0.832728</td>\n",
       "      <td>0.825464</td>\n",
       "      <td>0.827503</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'var_smoothing': 0.001}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.823078</td>\n",
       "      <td>0.828525</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.824052</td>\n",
       "      <td>0.826615</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022804</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'var_smoothing': 0.0001}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.822877</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.823850</td>\n",
       "      <td>0.826575</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>{'var_smoothing': 1e-09}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.822675</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.823850</td>\n",
       "      <td>0.826534</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1e-11</td>\n",
       "      <td>{'var_smoothing': 1e-11}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.822675</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.823850</td>\n",
       "      <td>0.826534</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.030201      0.004070         0.006997        0.002531   \n",
       "1       0.026600      0.000802         0.005602        0.000492   \n",
       "2       0.022804      0.000759         0.004801        0.001327   \n",
       "7       0.009139      0.007476         0.006263        0.007670   \n",
       "9       0.013457      0.002956         0.000000        0.000000   \n",
       "\n",
       "  param_var_smoothing                     params  split0_test_score  \\\n",
       "0                0.01    {'var_smoothing': 0.01}           0.825701   \n",
       "1               0.001   {'var_smoothing': 0.001}           0.825096   \n",
       "2              0.0001  {'var_smoothing': 0.0001}           0.825096   \n",
       "7               1e-09   {'var_smoothing': 1e-09}           0.825096   \n",
       "9               1e-11   {'var_smoothing': 1e-11}           0.825096   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.824491           0.829131           0.832728           0.825464   \n",
       "1           0.823078           0.828525           0.832324           0.824052   \n",
       "2           0.822877           0.828727           0.832324           0.823850   \n",
       "7           0.822675           0.828727           0.832324           0.823850   \n",
       "9           0.822675           0.828727           0.832324           0.823850   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.827503        0.003047                1  \n",
       "1         0.826615        0.003395                2  \n",
       "2         0.826575        0.003492                3  \n",
       "7         0.826534        0.003536                4  \n",
       "9         0.826534        0.003536                4  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_).sort_values(by=['rank_test_score', 'mean_fit_time']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the highest result was achieved when using a var_smoothing parameter with a value of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8852723604572966\n",
      "Precision: 0.9004646979463349\n",
      "Recall: 0.9694964493221433\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=46)\n",
    "gnb = GaussianNB(var_smoothing=0.001)\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after hypertuning the `var_smoothing` our model's performance improved significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Naive Bayes with HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Split the dataset into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingVectorizer = HashingVectorizer(n_features=20)\n",
    "vector = hashingVectorizer.transform(df.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hashingVectorizer.transform(df.tweet.values).toarray()\n",
    "y = df['is_hate_speech'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Create the Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6 Calculate the metrics (accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.828782784129119\n",
      "Precision: 0.840033153750518\n",
      "Recall: 0.9814396384764364\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.7 Hyperparameter tuning \n",
    "\n",
    "We will try to tune the var_smoothing parameter. This variable artificially adds a user-defined value to the distribution's variance (whose default value is derived from the training data set). This essentially widens (or \"smooths\") the curve and accounts for more samples that are further away from the distribution mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "[CV] var_smoothing=0.01 ..............................................\n",
      "[CV] ............................... var_smoothing=0.01, total=   0.1s\n",
      "[CV] var_smoothing=0.01 ..............................................\n",
      "[CV] ............................... var_smoothing=0.01, total=   0.0s\n",
      "[CV] var_smoothing=0.01 ..............................................\n",
      "[CV] ............................... var_smoothing=0.01, total=   0.0s\n",
      "[CV] var_smoothing=0.01 ..............................................\n",
      "[CV] ............................... var_smoothing=0.01, total=   0.0s\n",
      "[CV] var_smoothing=0.01 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................... var_smoothing=0.01, total=   0.0s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=   0.0s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=   0.0s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=   0.0s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=   0.0s\n",
      "[CV] var_smoothing=0.001 .............................................\n",
      "[CV] .............................. var_smoothing=0.001, total=   0.0s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=   0.0s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=   0.0s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=   0.0s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=   0.0s\n",
      "[CV] var_smoothing=0.0001 ............................................\n",
      "[CV] ............................. var_smoothing=0.0001, total=   0.0s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=   0.0s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=   0.0s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=   0.0s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=   0.0s\n",
      "[CV] var_smoothing=1e-05 .............................................\n",
      "[CV] .............................. var_smoothing=1e-05, total=   0.0s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=   0.0s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=   0.0s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=   0.0s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=   0.0s\n",
      "[CV] var_smoothing=1e-06 .............................................\n",
      "[CV] .............................. var_smoothing=1e-06, total=   0.0s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=   0.0s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=   0.0s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=   0.0s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=   0.0s\n",
      "[CV] var_smoothing=1e-07 .............................................\n",
      "[CV] .............................. var_smoothing=1e-07, total=   0.0s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=   0.0s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=   0.0s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=   0.0s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=   0.0s\n",
      "[CV] var_smoothing=1e-08 .............................................\n",
      "[CV] .............................. var_smoothing=1e-08, total=   0.0s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=   0.0s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=   0.0s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=   0.0s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=   0.0s\n",
      "[CV] var_smoothing=1e-09 .............................................\n",
      "[CV] .............................. var_smoothing=1e-09, total=   0.0s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=   0.0s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=   0.0s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=   0.0s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=   0.0s\n",
      "[CV] var_smoothing=1e-10 .............................................\n",
      "[CV] .............................. var_smoothing=1e-10, total=   0.0s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=   0.0s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=   0.0s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=   0.0s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=   0.0s\n",
      "[CV] var_smoothing=1e-11 .............................................\n",
      "[CV] .............................. var_smoothing=1e-11, total=   0.0s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=   0.0s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=   0.0s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=   0.0s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=   0.0s\n",
      "[CV] var_smoothing=1e-12 .............................................\n",
      "[CV] .............................. var_smoothing=1e-12, total=   0.0s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=   0.0s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=   0.0s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=   0.0s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=   0.0s\n",
      "[CV] var_smoothing=1e-13 .............................................\n",
      "[CV] .............................. var_smoothing=1e-13, total=   0.0s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=   0.0s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=   0.0s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=   0.0s\n",
      "[CV] var_smoothing=1e-14 .............................................\n",
      "[CV] .............................. var_smoothing=1e-14, total=   0.0s\n",
      "[CV] var_smoothing=1e-14 .............................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................. var_smoothing=1e-14, total=   0.0s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=   0.0s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=   0.0s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=   0.0s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=   0.0s\n",
      "[CV] var_smoothing=1e-15 .............................................\n",
      "[CV] .............................. var_smoothing=1e-15, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': [0.01, 0.001, 0.0001, 1e-05, 1e-06,\n",
       "                                           1e-07, 1e-08, 1e-09, 1e-10, 1e-11,\n",
       "                                           1e-12, 1e-13, 1e-14, 1e-15]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n",
    "}\n",
    "clf = GridSearchCV(nb_classifier, parameters, cv=5, verbose=2)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_var_smoothing</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030201</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'var_smoothing': 0.01}</td>\n",
       "      <td>0.825701</td>\n",
       "      <td>0.824491</td>\n",
       "      <td>0.829131</td>\n",
       "      <td>0.832728</td>\n",
       "      <td>0.825464</td>\n",
       "      <td>0.827503</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'var_smoothing': 0.001}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.823078</td>\n",
       "      <td>0.828525</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.824052</td>\n",
       "      <td>0.826615</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022804</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'var_smoothing': 0.0001}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.822877</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.823850</td>\n",
       "      <td>0.826575</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>{'var_smoothing': 1e-09}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.822675</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.823850</td>\n",
       "      <td>0.826534</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1e-11</td>\n",
       "      <td>{'var_smoothing': 1e-11}</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.822675</td>\n",
       "      <td>0.828727</td>\n",
       "      <td>0.832324</td>\n",
       "      <td>0.823850</td>\n",
       "      <td>0.826534</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.030201      0.004070         0.006997        0.002531   \n",
       "1       0.026600      0.000802         0.005602        0.000492   \n",
       "2       0.022804      0.000759         0.004801        0.001327   \n",
       "7       0.009139      0.007476         0.006263        0.007670   \n",
       "9       0.013457      0.002956         0.000000        0.000000   \n",
       "\n",
       "  param_var_smoothing                     params  split0_test_score  \\\n",
       "0                0.01    {'var_smoothing': 0.01}           0.825701   \n",
       "1               0.001   {'var_smoothing': 0.001}           0.825096   \n",
       "2              0.0001  {'var_smoothing': 0.0001}           0.825096   \n",
       "7               1e-09   {'var_smoothing': 1e-09}           0.825096   \n",
       "9               1e-11   {'var_smoothing': 1e-11}           0.825096   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.824491           0.829131           0.832728           0.825464   \n",
       "1           0.823078           0.828525           0.832324           0.824052   \n",
       "2           0.822877           0.828727           0.832324           0.823850   \n",
       "7           0.822675           0.828727           0.832324           0.823850   \n",
       "9           0.822675           0.828727           0.832324           0.823850   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.827503        0.003047                1  \n",
       "1         0.826615        0.003395                2  \n",
       "2         0.826575        0.003492                3  \n",
       "7         0.826534        0.003536                4  \n",
       "9         0.826534        0.003536                4  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_).sort_values(by=['rank_test_score', 'mean_fit_time']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the highest result was achieved when using a var_smoothing parameter with a value of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8282447881640888\n",
      "Precision: 0.8390075809786354\n",
      "Recall: 0.9824080051646223\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=46)\n",
    "gnb = GaussianNB(var_smoothing=0.01)\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after hypertuning the `var_smoothing` our model's precision and recall improved slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions\n",
    "\n",
    "We can see that from the three types of classifications we tried, the one with the highest accuracy was the SVC with a CountVectorizer so this is going to be the one we use. That is not very surprising because SVM Classifiers offer good accuracy and perform faster prediction compared to the Naive Bayes algorithm. They also use less memory because they use a subset of training points in the decision phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "How to Encode Text Data for Machine Learning with scikit-learn (article)- https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/ <br>\n",
    "Text Classification (article) - https://monkeylearn.com/text-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Credits\n",
    "The following Jupyter notebook was made by Karina Kozarova as part of the Artificial Intelligence specialization at Fontys University of Applied Sciences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
